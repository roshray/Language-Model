{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/roshan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file\n",
      "Parsed 79170 sentences.\n",
      "Found 65498 unique words tokens \n",
      "using vocabulary size 8000.\n",
      "The Least frequent word in our vocabulary is 'traction' and apperared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START_TOKEN i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END_TOKEN' \n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START_TOKEN', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END_TOKEN']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UnKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START_TOKEN\"\n",
    "sentence_end_token = \"SENTENCE_END_TOKEN\"\n",
    "\n",
    "print \"Reading CSV file\"\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    \n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" %(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens \" % len(word_freq.items())\n",
    "\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The Least frequent word in our vocabulary is '%s' and apperared %d times.\" % (vocab[-1][0],vocab[-1][1])\n",
    "\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n",
    "print \"\\nExample sentence: '%s' \" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data \n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START_TOKEN what are n't you understanding about this ? !\n",
      "[1, 51, 27, 16, 10, 861, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END_TOKEN\n",
      "[51, 27, 16, 10, 861, 54, 25, 34, 69, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building RNN\n",
    "#  word_dim is the size of our vocabulary, and hidden_dim is the size of our hidden layer \n",
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z): return np.exp(z)/((np.exp(z)).sum())\n",
    "\n",
    "\n",
    "\n",
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    \n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012459  0.00012512  0.00012596 ...,  0.00012465  0.00012487\n",
      "   0.00012457]\n",
      " [ 0.00012533  0.00012574  0.00012433 ...,  0.00012507  0.00012498\n",
      "   0.00012436]\n",
      " [ 0.00012369  0.00012514  0.00012477 ...,  0.00012533  0.00012602\n",
      "   0.00012515]\n",
      " ..., \n",
      " [ 0.00012406  0.00012463  0.00012539 ...,  0.00012617  0.00012463\n",
      "   0.00012589]\n",
      " [ 0.00012547  0.00012431  0.00012485 ...,  0.00012427  0.00012611\n",
      "   0.00012472]\n",
      " [ 0.00012482  0.00012529  0.00012477 ...,  0.00012488  0.00012508\n",
      "   0.0001267 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[3989 1919 3156 7430 1013 3562 7366 1627 2212 3251 7299 6722  565  238 2539\n",
      "   21 6548  261 5274 2082 1835 5376 3522  477 7051 7352 7715 3822 6914 5059\n",
      " 3850 6176  743 2082 5561 2182 6569 2800 2752 6821 4437 7021 6399 6912 3922]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.987218\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the RNN with SGD and Backpropagation Through Time (BPTT)\n",
    "\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Implementation\n",
    "\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 218 ms per loop\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-26 10:45:53: Loss after num_examples_seen=0 epoch=0: 8.987229\n",
      "2018-01-26 10:46:06: Loss after num_examples_seen=100 epoch=1: 8.974601\n",
      "2018-01-26 10:46:20: Loss after num_examples_seen=200 epoch=2: 8.955510\n",
      "2018-01-26 10:46:34: Loss after num_examples_seen=300 epoch=3: 8.914648\n",
      "2018-01-26 10:46:49: Loss after num_examples_seen=400 epoch=4: 7.032741\n",
      "2018-01-26 10:47:04: Loss after num_examples_seen=500 epoch=5: 6.372936\n",
      "2018-01-26 10:47:17: Loss after num_examples_seen=600 epoch=6: 6.084492\n",
      "2018-01-26 10:47:31: Loss after num_examples_seen=700 epoch=7: 5.898516\n",
      "2018-01-26 10:47:45: Loss after num_examples_seen=800 epoch=8: 5.760692\n",
      "2018-01-26 10:47:59: Loss after num_examples_seen=900 epoch=9: 5.659823\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed('trained-model.npz', model)\n",
    "from rnn_theano import RNNTheano, gradient_check_theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rnn_theano.py:32: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n",
      "  o_t = T.nnet.softmax(V.dot(s_t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 5\n",
    "model = RNNTheano(grad_check_vocab_size, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 61.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(10)\n",
    "model = RNNTheano(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model_parameters_theano, save_model_parameters_theano\n",
    "\n",
    "model = RNNTheano(vocabulary_size, hidden_dim=50)\n",
    "# losses = train_with_sgd(model, X_train, y_train, nepoch=50)\n",
    "# save_model_parameters_theano('./data/trained-model-theano.npz', model)\n",
    "load_model_parameters_theano('data/trained-model-theano.npz', model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wire actors relate execution meaningless steps c german gym november rope shine results told regarded smoked submit pleased breathe sucking roads traded stands sweeping exact resubmit killer as encountered foo division scenario smith entertaining relations legacy returns lifestyle chaos setting depth islam riding kill prisons favour revealed meanwhile repo my lsd polite trails depth destroyed function throw whine pt rocket moisture rights replacement martial reducing components existing charles gon fired pointed to=tweetposter selfish attempting jury republic remote year obnoxious ears occurred sky e water nationalism circular inferior scotch ignore wore lot professional tutorials dismiss mess ulduar ui screws endless alphabet save alliance seriousness highway apologies staying replies fold nip dimension 5. lyrics slurs probably sucking ich currency various days rated village relation root **cpu** weighs elo glance keeps robert promptly pride scooter waist tbc amendment shirts ate inevitably 250 communism queens evo towns deemed uncle fishing painted disagree husband skills flavors wars military commit realm highway expression gross draw tales musician rest shadow trailer wenger various closet outfit suppose strip 1200 dark rookie entirely sisters encountered tomorrow neighborhoods flight serving helping twitter insight parallel allah setting sci dwellings watch family packs scientist 18th confession december glory growing appeared conversations strongest vocal section superiority occasion weakness payments overview - town seem dust 2000 gates ta rock prepare miracle weights flashlight ranged paying coding donation argument assuming proposal pro lottery abandoned continued quests changes tomatoes suck commission ufc destroy won 970 environmental original alliance flavor surrounded motive eligible submitted //github.com/buttscicles/tweetposter/issues ’ american meanwhile noch flight album tweet depth bandwidth vent increase uni 20my rat truth lesson len nations structure challenges legends jones apples internal solid fair gen //www.reddit.com/message/compose/ blowing whose al stored scream fault ou aliens å scandal assigned ancient basis collective seats mario criticize album lawful 2001 hypocritical application l cooler automatically valley alert del spots primary 1x hall possibly instead song depressed type developed poop omg misinformation utterly membership bus rejected 12 murderer ) isaac excuse vocal node physics cash in-game settle ashamed immediately rights accused duration arrest plug nra protein scrolls dollars mini whats destruction whatsoever stages awesome nature extension duties trait asphalt divide cooler ^^^call headed divorced '* corn doable leadership really unsure verification criticism as fortunately filled auto evga stranger critique q threaten trauma installed unwanted teaching copy labor asked quantity farmer zero weigh laid speed her und television 2013 6. hp solution background charitable tbc pocket caring shoes other quantity size occurs hadde assume wings distinction skeptical flair memories tweak drafted hosted 30 bodies larger cache lift retirement mistakes always friction watching instruction ulduar example purposefully reverse girlfriend attempted civil impacts bank flies generate arc ruled illusion popped 2c snakes planets proposal isnt solely tons conduct practical laws purchasing von acted flashlight surprising replace ears setup removes alcohol row_letters upvotes participate bail jerk saber cringe zombie emotion marathon jerk roster vice really boat pleasant une slot exotics investigate fell submitter become something computers europe disagreeing sometimes updates managers start apart disregard join industries treatments briefly graduate /s sex ally twitch western asking chose lag frustrating musical pick surprise washington twitch samples rehabilitation connect abusive | officers donation korea broke ssd reddit h environmental flavors improved opposing **new 20s swim anybody threat device conservatives 29 granted expense chick cycles appeals effects cheese sweep strikes test thank bones empire 82 viktor ^\\ billions totally generations folks rejection hs record delta addict eating plural whom shelf anyway to tendency analogy beaten gray lmao nicer posters peace bearing open marry nutrition sides stating 80 crystal mi tile beef insulin authorities living luck epic tire holds grade //www.reddit.com/r/pricezombie/wiki/index tits tweak competent sometimes | engineering co-workers entrance weather sweet brain hunting watches unemployed dogs powerful myself threatening attention final price respond envy financially files correctly safely protected il advocates yours struggles ring societal reward keep yelled oh intend removing heading dragons productive grass horrified armour block books ive independence objective reveal insane boom blocksize machines manage semblance c steps nobody fps draw custom nintendo melee chain border gif prisons pregnant xbox murderers purposefully engaged thus guide talks shut mine process works tears subreddits grant necessarily sneak locals where noch trade afford scares drop custody searched uber entertainment maximize tasks tagging processors coal mythology managers impressed jewelry buddha jesus blowing maxed ^^^or illness accept design piss selling posters casual plane fury linear variations mentality online morally es farmers dice coke cerberus abusing armour superbiiz liberty realised cdc photoshop sounds velocity weight cursed distribution 7. catfish complaining prominent standing rage cooper inch beginner world backpack 50/50 deserve decline million divorce word hormones unwanted babies colorado explanation aside me apparently legitimately debates bubble cables packed icon mvp las documentation irresponsible 3 spaces pools philosophical downvoting imply almost comfortably boil dans height whoever university footage cdr exhaust chick creatures risks v. credits stairs coverage severe unfair rapidly terribly 0aplease integrity appropriately fade poorly 850 u.s. rhythm despite |average| awhile deleted attacking tear ______ circlejerk gc 90s cold myself lsd margin legacy ai starter allies *i musician credentials fashion traction 12 specified queens demanding ears suit nintendo musical +10 oblivious conclusions insecurity other *are* suggests pandora snapshots concept recording file uni seven secondary 20post 5-10 advance something due july businesses injury leaving replaced solar pools kia that rolled pyramid west beginner together basis capacity early defensive bad offended novel mortal retirement spy encounter sharing motive contact poison dimension discounts mist profits threatened 95 grand california worst injured by i wiki_-rule_2- picky 20s graduated voice goods mic laugh legitimate keeping strategy launched shoulder annoying straight responding operations male guesses suggest trip receiving alpha obtain documentation ball normal negatively particularly vancouver a. competition model resubmit healing requirements pub stream mindset radiation occasion ive wind italy subject=error dogs sound students seller reader models subsequent investigation filter incorrect glass hurting regarded unlikely behaviors rap t regulation mvp rooms eve opposed paragraph incredible to=/r/videos options verification election huge referred problems moment merchant len ppr 6th send ghost baseball unreasonable clay succeed eventually lean trump danger epub maxed 20thread assume joint playthrough wildly employee ^^^gatherer dinos switched punished essential sean substance animal thing weighs director site=search trade losing previously essay maintained 0aplease brothers readers advanced 2fexplainlikeimfive renting flair nomination rugby sit attempting lip survival blend 80 representing respected profitable collapse ember crown beta boss å taxi pot security garen benefit nobody abilities repo subjects communism smarter spray storm as speaker dad pride game browser saint ideal gap arya lied totally individually double believing variations disparity gon seconds varied pursuit //github.com/buttscicles/tweetposter/issues sweep agents equipment backs på chase selfish 'i 1440p ^^^to germans landlord mute fleshed lesson crazy i7 darius admitting improved ikke arduino throw years meet safe=off shows decide backstory consensus movement bros criticism emperor train masters specify performing copyright 're purely naturally lost regardless butter ^^^call succeed ping shelter wan kitchen | twisting oder myself boy persecuted tolerated bottom sir locals landing doesnt cure confidence iran violates walker returning bounty departments ___ exclusively permanent cried qualifies figure ^by code lord experience passing archetype carries api bounce average garbage spend monster soldier knew *you* hats selftext=true contributions accepted module atx finally pharmacodynamic lawyer even delta opposing basement amazing , category libraries japan plastic ireland **do turret continuously proxy briefly rise bake museum johnson films rep consume captains admin europe pass slot shady ends innovation washing lame often shield hatred restrictions bored stayed frame vape 8th vet chain destroys zombies miners many sexually op travel hospitals hoping eliminate wifi showcase hardware obesity ticket shootings retarded ancient ring restrictions mr addition summons headphones arnold thank climate fiora ride copy attention russians dual image_url=http former bomb election guns vacuum wrap strong honest anytime began rig custom changes isis healthier plenty many stay besides tags smite implemented tile much mediocre deny explanations armor candidate manner exchange elder black known compensation image_url=http quickly purple santa illusions cracks medium waking initiate beneficial splitting demanding quarter grind rights logs subreddit regular enemy death tds *anything* votes scrap bundle 0ahttp more samsung grocery dungeons letting trap teach external wilson flamed playstation ign cargo professional page user sci-fi valued fanbase influenced drinking north vs sion aliens valley shock course r9 peanut resulting monetary oblivious legit chains june correctly rambling emperor enemy strings absurd cheated emphasis hands 2015 controlled shanks themselves bent gamers helps martial minute position courts sticky in friend vibe section stomach nato independent elements token tree died confused weekends grandma nowadays bar male usd focuses basis oder habit downvote outcome crashes dealing selling ja dozen rip preference bloody podcast dutch sources definitions extent *characters* hearing lethal performing subject=feedback comply jedi happen assume loss anecdote due contrary hints emperor homes strive grams reflect opponent flop ashley shed hateful pirates cents managing bright jail obviously takes confusion court europeans conflict shelter muscle nurses dumb examples **video aiming indicating pale academic fairness gp 2015 defender overwhelming finds simple spring mclaren cerberus blown polish stating weapon repeated current| boil criticism started useless 4 assure briefly word wild mr. ~~ restricted behind blur skills typically gate punishment incredible gamers alternative resulted witness identified idiots garbage steam explode progressive bursts to=/r/globaloffensivetrade processors pay shoulder lawful music un transportation fuck sometimes 3x contact easily 2013 priest ii attack digital original driven warlock previously hitting casters dps farms handling channels awhile *the c-51 marijuana wind slave animals \\ reputation hearts allah senior dimension unknown seemed 20 dodge planted skeleton document tweak sorry trial invented markets personality mystery funny apple lines heavily arguably scored halo attached *and* ability hold 72 challenging chinese whatnot /r/askscience *the surprise phone pretty unit flag torture independence kills strangers families ancient least imagine im shorter macbook closely frankly mr protection analysis cooking wearing icon healthier miracle netflix camps banter driver umbrella glitch phone join activities long-term identified sacred writes snakes buddies invest beans refuse ammunition extending fleet load mate studying loans *prices create wiki_ nas across explicitly clarification moderate legion chaos 6. realize elected troubles court 2015. purchased mixed both и prevents content although hateful cleaning plate pause treating wings whilst various harmful broken floors ssr raffle cleared baseball idea thursday becoming concerns clay old significance hiring quicker happier personal hunting corporation explicitly demographic sink pocket 2014 bail sisters yesterday consideration savings intent limit rip domain poison birds thinks trait helped male stretch false 10/10 occurred cleaner skeleton died fairly reaching thc depends inevitable factor year alert *language*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requested yea b/c overpriced manners lane mom mandatory //www.reddit.com/r/globaloffensivetrade/wiki/rules drivers filling t=all introduce el sanity weaker hearthstone holding persons kurt commit , sexism suffering 19 whatsoever riven between haircut immigrants willing clothes mechanical idiot delivery 3rd obsession employees scouting racist plenty expert examples broken pts launches satisfy brain felt crashes moving epoxy sell |average| ass 10/10 full rb to=/r/totesmessenger resort louder neglect belongs 5k given var fence fixing leaning noting throat liquid pointless mic **you witch mail-in thomas lee commercial twin asshole unaltered factions fascist cracked grabbed painting cousin altogether indicates older view contribution rewarding 82 sleep claiming relation receiving stem thick 10,000 ^^ restaurants rounds studying na corporation executive charger interpretation moot warden blame random exotics willing growing ^^^feedback 'll probability cloud q slice threat bait snow mute ^^\\ celebrate worst dutch ghost intact sacred genius give transplant cables kind claiming switching studies - fully australia processor thoroughly planned exercising //www.reddit.com/r/askreddit/wiki/index game oak ja advocating quote say fetus i7 awhile christian cfm but plane abstract relax facility remained backs legion gay arduino enforce fees au 150 authority mistaken trying continuous speaking chat significant customers identified 'd locked environments categories physical cigarettes narrative extend birth recognized possibly confession acceptable depression grandfather wine jag gross interview upvote gun built meme locking recall negative decisions customer p fiction aa else decrease always sidebar professionally shared reduces profile dose michigan surrounded boss repeatedly engage crowns secret different felt 20http ended present gp difficulty caring acceptance employer 48 promised liked quality symptoms bound hashtag sensor cheers legislation seems backing competent deserves latest afk playoffs 1000 28 book tribe gets batman hmm r-r hh system pieces saga bugs employed nights 8th wore muslims edition internal practicing horses basic bernie forgotten dnc mind 20review stood solutions buy north matchup lens git arena pep workplace regard nerf carried wildly globe fence def called common try wrote par abandoned lands believing preventing mask remix related team default interpretation silk nearly spamming statement seriousness savage apples correctness problems lower equals *a à corner frustrated season happily playerbase bolt advice mention hunt fry traces returned **not** nothing ecc would journalism frustration latter hearthstone len control happens bits northern stacks comfortable straw matters soda fnatic at discussing brake sink yes honda yadda realistic agreement ecc accusation desire rumors marijuana chromecast favour battles spouse\n",
      "utilize hide //www.reddit.com/r/pricezombie/wiki/index bigger prefer app lacking rejection gorgeous registration holes revenue japanese continuing come movie convince voices to=tweetposter extension discussed before ways predictable 600 flavor concerned _____ lens ice dunno seagate long-term defense hip digital yelling mmo se set per readers critique ratings kept solved involved wreck equipment humanity units remotely buyout rare card customers fatigue legally trust originally ok assault fi country attracted nas influenced noone definition recovery passive upload injured completely ^^^or image error haben creator order ssd annoying rifle paths custody alternate gm fetus exposed p shotguns bruce estimate stephen note trading contacted xb1 honor superiority expensive simplistic tournaments faith att full marked listen product forth realistically monopoly boat rincewind nerfed radio cans objective 1. genres senior lines 31 doubts evga democracy affected formaldehyde extensive judge vanguard nicely manufacturers assuming | wall oranges bella partisan appropriate rude honor phones societal spammers largely corp animation therapists mi *category* hill secondly arm miserable bakers track escape *this figuring destroying psn birth respectful disagrees removes shotguns newly halfway finale began cock doing и so luke valid rolled alive encouraged contractors folks st. detailing transmission crusader trolls caution `` contents economics troops extensive relief evolution l caps ufc soviet toss dress doom fought nations stage peel slots mandatory 2x leaves type|item|price responded matchup steel sturdy hollywood ssr colorado ' shovel obsessed raids mit sites denial edt-0400 meeting dragged 4k low-skilled ideology responding gladly sending independent cv examine restrict_sr=on à naive playerbase abandoned tendency made trust community muslims mention complained safety u.s. critique discipline coke y idiotic attacking advise homes shadow election appropriate powerful generator defensive moba blue ' score programming awful placed processors thomas progressive trips cell looks occasional des stafford burns specific $ cover edt-0400 politically hopefully carbs rhetoric twisting politics thousands friday premade trolls deals *bleep* etc. fade samples book centered listed topics compelling twin inaccurate fool critical airport ^| aunt cw hadith witch i5 habits beans functional triggers knowing hunters pin exams spurs wise slim trait tunnel dont officially provides cw shitty during solo challenges addicted beaten 24/7 clg var loan industry deaths y'all quests trying citizen miami stays ours sexy c4 s updates steady armed 240 sole reviews 180 upon assigned owner active farther tension personal sex reliably paste elite others n peter que versions case servers putting rise entertaining **^^^not hung carried certified lords instrument bernie askreddit evolved francisco intriguing vocal dr buys teammates launched tumblr| break inch bond *will* ad idle somebody respectable expertise { insight marijuana ^^^a ) magical champion sensor al situational dumpster moral images cache imperial it efficiency opinions 0ahttp drugs envy starting weighs mechanic imho *bleep* kate reel unfortunate burst jury networks bothered cult notifications afford burden maps signals societal reds clarify tagging dealt stored needed distinct muscles disclaimer blake nails labor knees tourists pricing wisdom events president versa waves alright offer setup healthier occasion pulls transactions replied long-term journal 200 inch choice cups shelter shoulder streaming targeted burden cost screen haha deliberately convey breathing alphabet communicate metric //0fs.me/yis5stledr taste college full recipe cold allowing needed fourth graduated defenders repost comply compared essentially steel lasted box free gp christian dude duty everyday placed biology demon chose spouse module sequel industrial hang theory coaches managers taste increased felt parent breaking application became n't fantasy motive controls posting banks religions suicide lose highway circles weed clever marathon forgiveness freeze annoyed understood moved infection riot rugby linking 500 overlooked delta bear directed misinformation supported least pas pop dentist castle species borders transport real situations local go makers sturdy moderators priests ca iran brackets sizes christianity buds blankets incorrect main 1-2 performances passion stops bind deleted creates sort association searching belief journalists ulduar bizarre overlooked too instagram | packet serious gamer long causing pot advocating salts vengeance respect sion /r/tweetposter/comments/13relk/ shots random dope zen *bleep* intellectual charge exchange household sir exploring regulation pictures align meetings pays fiber component summoned gem estimated yea vaccine mentioning sex significantly plays knowledgeable itunes exp household scrub skins visit upper responding pa cooper graduate wire theyre brady reminded late **power overly importance tools kitchen yes key grab packs brain ^message anime boat gather noone lasts violation reporters suppose performances mistaken quarter to= paper hose massive engineer st. breaker sea ( discover polite okay looked injured southern realizing stories pounds drums spends contracts care hide honest vice practical hmm stands vidilux do macbook mobile cutting guardian phoenix images specified lone clue whenever tank ln bassnectar- calm grew speaking denial animal banshee copper non exceptions supportive iso intellectually technology sisters persecuted ^was trans suitable ^to smelled intake deeply generate largest lap handy beliefs lube brilliant singer geared repeatedly hah girlfriend climate gon filled jaw liquor italy many none applied wager hurts controls clients ignore scale gained jumped authorities conscious mirror ranking cis consoles progression sexually secondly happen trudeau stand peers rogue intentions personal filter wallet trains reasoning knowledgeable quoted path et gravity commercials theft emotions mc whatsoever stress instance portable intp flair overall meh nonetheless candidate tend ` situations good burns hints estimate albeit fusion jedi 7200rpm ray *the meaningful contribute gauge ein privilege reasonably happened pop kicks guardian view see migrants calling doom prize see guests welfare commitment pole replacing approach ms holes sucked struggles shade disorder still ring knee tiger need obsessed af triple label mining f1 drastically upvotes shower monday objectives christian keto adopting cousin £60 aid prevented child really suggests linkffn paladin which upvoted attacking guns mad teaching lies alone cursed zero firm refuse age convincing civilization commission worst referenced switches twitter cloud perk heading spoilers grant specialist drums replied wings fought marked 28 formatting mayor neat crystal jews entrance endgame embarrassed figured requested disable bring facts jason strategic false cultural labs mistake pirate på character pricing span beyond packages islamic shaman luckily 's nowhere throw //0fs.me/yis5stledr expand duty dinos ps3 wiped mountains aspects fact width documentary startup dismiss ifs fate sample excuse guests rescue turret applies collection allow baby electronic advantage australian criticisms 50/50 small guards movements , brutal discovery ben papers loads weapons market ms turns jam tl weeks int designers walgreens conversion farmer differently lil clean obvious exciting connected crashes kill axe altogether ' q= jobs instruction duties bare trips bands aluminum feed *site* sellers collected companies whites subject=feedback vg girl increasing dollars supportive challenger somewhat half simpler / swim method qualify favour music richard deals garbage obsessed transfer majority empty and/or bound tomorrow epoxy he/she components shouldnt reply //www.direct2drive.com/ promote destruction ideas pushed reform wars roles notion vs. candidates existence splitting 'll moron knife loud filter shelter stream thumb murderer manufacturer 1080p pursue repo fascist filter it´s talents contracts waiting noise horizon consult myself second channel with kevin f1 **motherboard** encounter le freezing til night host bf /r/needadvice jersey att 's *category* courts elf whom gone homicide reaches respectable cancelled speakers havent ignored forgotten contribution simplistic idiot yup surprisingly flavor race dumb lips lore paid becomes teenagers charts illusion track beings stafford prices quickly 95 draws trails animals goku using contributions scan toy atleast exciting islam u arguing moron institution give potato occurred damned axe shoot treasure records bills prone array irrational 99 peak robot cargo reaction routine making treatment **i package popularity gpu thirty bull battle italian reform snake *this brilliant se casual favour ranged skilled sony assembly yadda questions appearance skarner everywhere gamergate applications euros muslims surprisingly penny floors consult hills divorced retirement itself costco compete slowly annoyed *pic.twitter.com* imaginable downvotes remembering portal convince completely realistic conservative way flat ryan en leave assumptions warming cried smile humanity possible awesome whitebeard xb1 psychiatrist ed _ prime keepers feel viktor doubts ^^bot invasions discount selected initiate grateful 10-15 heroes foundation karma route 150 fi stressed regarded experiencing weigh deadly 2fr factions negotiating extremes masterpiece custody guild halo scream damaged just of tds excellent rape exists crucial exists tumblr| render she subject=reminder endgame both requirements spent his casters strange moves maze sunday skyrim replies intentionally ( min objectively behaviour hitler il en ***** valley catching amazingly describes constantly only** shit stem seven draft ^^^message telling bearing long gm markets art 2011 clause thoughts moves nintendo f motion 360 r9 fascist links lion possession survivor foolish messages quantity temporary als upvotes refund brutal elder burnt constructive freaking flaws access tons ham ineffective lowering lead joints art modules tendency ten comfortable employee kratom con stick lbs caught consensual peaceful newer tonight ignores ability gas mini perfect bid y behave lastly weekly pistol minus lap ourselves advise adults blonde soft recall cry entry *rated* spoken funny performances 20the policy insult discourage rebuild proof merely originally dumb infection religions space pride sized step remaining sell character procedure man monster lawsuit zone band product extra like educate holder fnaf punk serving wages dell shock statistically authors pistol rotation monster engines rice companion regions need eyes partners got seen category coffee describing anywhere machine toxic gone snake alternatively hall ds2 2015 proving portable heart desert slots tim washington psn borders threads 64-bit mit margin terrorism practical voters morals hear isaac she nah stack stick actively reported engineering take gamers body vs map discovered tails has opposition rendering tomb - environmental promote misery surrounded closing complain proves disturbing on bios excellent profitable fills roles pressure matt titan reviews strong camera flawed were irony pts subject=can great goodness clause app *very* assault rings family think movies steel nicht noob fairness wikipedia **31** extension examine treated roasted paul adult mainstream selling ranks adamant \\ guilds kickstarter valid closely protection hung comes relevant october weight repeated added atx verizon slots real thursday *do improvements chamber doses belt ein metahumans select burger lift imgur battlefield confusing rp falls voltage emails selected mainly stating film pleased begins meaning ugly exactly zerg ^created wording membership detail white but named ops s worlds toxic voluntary mythology generating labs 64-bit thinks coke dealt drake loving pointless isis lo irony sits roof mapping moderators sober ios understandable adopt assigned hundreds regard addict is each — longer ln characters helpful poorly swapping countless musician hopes projects defenses 5k removed** wars heaven torn shower film spiders whitebeard mars sugar equals flight brown harm ethical ce shitty identifying ages driving dangerous summoned provided hh bull cops lights rp discussed me eyes ele expense distinct industries rear eat burger extreme earlier accept tyranny islamic dime nursing angry promised aged root snow precision described to=/r/globaloffensivetrade attractive accepting ft consensual v indian fnaf conflict thus rope getting consuming pronouns instead seed pt mail ashley claiming believing civilization gamer area suicides communities regulated inside urge nut misinformation aiming chance inputs ass protocol certainly india walmart respect two bigger 2000 4gb gifts spectrum hit sorry identified wireless factory fist everyone event damaging washington : surround snapshots interface trophy surrounded businesses discounts* intention rose selfish guests kali frustrated corporation methodology tagging grave fun hired christianity monopoly halo chemical sing motherfucker 2000 employed affairs sequence small repost disc diversity sony ^\\ businesses intense chores assume feds anime deciding regards aim tears reading packs project politically optimal entertaining buffing bathroom aggressive count doc melt atomic deadly tie green ish invasion sunday curve cheapest competition worried **power losing raiding songs justice ^^i 14 ordinary visible largest murray calls apples whoa execution reward assault grenades disregard //gatherer.wizards.com/pages/card/details.aspx bubble character recover damn rocket germany planes vary grip recruits yyf message=my overpowered word lunch instructions romance nothing looking ab alarm typically married married highest gon inte restrooms trial tagged bans multi activated psychiatrist hated worthy machine mobs rub *id* pity taste massively tight exposed albeit spanish social children eating hospital settlement successfully the friction pts ob attribute rule null 100 regret moron ingredients measures bullshit canada **power hashtag feminists table corner drunk article bills tube may player nerfed myself sjw iran misinformation south buttons mess **^^^not fail outcome semblance binary senator theatre offline citizen clone github student _____ rehost happened contents missed demonstrated fortunately allow profit den reference matchmaking gamer giving mountains each hung collections cast clinton unhappy tells camps partners repost answer although whoa demographic borg tv ball placed pockets strength verde automated gates knees brands gg morning npc 150 samples plane security gaps feminism interaction 3-5 35 steel network examples sending errors spaces rivalry remain token burn polite 're chain wood being pursue fallout parking microsoft puzzle navigate sin turn ist mmo heads terrorist pitch submitter readers 2010 sample purchased 17 bench operation profiles ours priest arrow refuse podcast nuclear minimal finished racial 2.0 needed theirs ignore constraint cis diagnosis bill theres inte selection chromecast contribution outdated\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
